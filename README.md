Build an Uber Data Analytics Dashboard 

This data engineering project idea revolves around analyzing Uber ride data to visualize trends and generate actionable insights. This project builds a comprehensive ETL and analytics pipeline, from ingestion to visualization, using Google Cloud Platform. 

Project Idea: Start data engineering pipeline by sourcing publicly available or simulated Uber trip datasets, for example, the TLC Trip record dataset.

Use Python and PySpark for data ingestion, cleaning, and transformation. Key operations include handling missing data, converting timestamps, and categorizing rides by parameters like time of day, trip duration, and location clusters. Store the data in in Google Cloud Storage to ensure scalability and reliability. 

For data transformation, deploy Mage on a Compute Engine VM, where it performs ETL processes like cleaning, aggregating, and enriching data. 
Store the transformed data in BigQuery, Googleâ€™s serverless data warehouse to enable high-performance analytics. 

Finally, you can deliver visualizations and insights through Looker, which integrates seamlessly with BigQuery to create interactive dashboards. This architecture showcases a modern, end-to-end cloud analytics workflow.

Phase 0: Project Bootstrap (Code Foundation)

Goal: Get a stable, reproducible local platform running before writing any data logic.


Next Step (Phase 1)

Once you confirm:

Docker is running

Airflow UI loads

MinIO UI loads

We will next:

Design Bronze ingestion contracts

Write the first ingestion script

Create the first Airflow DAG

ğŸ‘‰ Reply with â€œInfra is upâ€ (or paste errors if not).


ğŸš€ Phase 1: Bronze Layer â€” Code Development (Apple Silicon)
What We Are Building (ONLY Bronze)

Goal
Ingest raw TLC Uber/Taxi trip data â†’ store immutable raw data in MinIO (S3-compatible) â†’ register ingestion via Airflow.

NO transformations yet
NO dbt yet
NO analytics yet

This matches real-world medallion architecture.

Option A: Fully Dockerized Spark + Python

âŒ On Apple Silicon:

Slow

JVM crashes

Hadoop native libs pain

Volume permission issues

Harder debugging

Option B: Hybrid (What Weâ€™re Doing)

âœ… What runs in Docker:

Stateful services (DB, object storage, scheduler)

âœ… What runs locally:

Spark

Python ETL logic

This gives:

Faster iteration

Better debugging

Lower memory overhead

Cleaner stack for a Mac Air

4ï¸âƒ£ Why Airflow Is in Docker but Code Isnâ€™t (Yet)

Right now Airflow:

Orchestrates

Triggers commands

Does not execute heavy Spark workloads itself

Later we will move execution into containers (Kubernetes-style thinking), but:

You do NOT start with that complexity

This mirrors:

Local dev â†’ staging â†’ production

EMR / Dataproc / K8s job execution

5ï¸âƒ£ This Matches Real Enterprise Patterns

Hereâ€™s how this looks in real life:

Layer	Where it runs
Airflow	K8s / Docker
Spark jobs	EMR / Dataproc / Yarn
Object storage	S3 / GCS
Local dev	Laptop

Your setup is a local analog of this.

6ï¸âƒ£ When We WILL Dockerize the Code

Later phases:

dbt â†’ containerized

Spark â†’ submitted via Airflow operators

CI/CD â†’ Docker images

But not on day one.

Trying to do that early is how projects die.

7ï¸âƒ£ Portfolio Explanation (This Matters for Interviews)

If asked:

â€œWhy didnâ€™t you run Spark inside Docker locally?â€

Your answer:

â€œFor local development on Apple Silicon, I separated stateful services into Docker and ran Spark natively to avoid JVM and Hadoop native library issues. In production this would map to Airflow orchestrating Spark jobs on managed compute like Dataproc or EMR.â€

That answer wins interviews.

8ï¸âƒ£ Summary (Burn This In)

Docker runs services

Python needs client SDKs

Spark is better native on Apple Silicon

This is intentional architecture, not a shortcut

You are building this the right way

CSV

âŒ Spark reads every row, then filters

Parquet

âœ… Spark:

Reads only fare and pickup_date

Skips entire row groups using metadata

This is why Parquet is data-lake gold standard.

6ï¸âƒ£ Compression Without Pain
Format	Compression
CSV	gzip (whole file)
Parquet	Snappy/ZSTD (per column)

7ï¸âƒ£ Evolution & Compatibility
Feature	CSV	Parquet
Add columns	Breaks consumers	Safe
Rename columns	Manual fixes	Managed
Partition aware	âŒ No	âœ… Yes
Schema evolution	âŒ	âœ…


âœ… What You Have Achieved

You now have:

Proper Bronze layer

Raw immutable data

Spark ingestion

S3-compatible object storage

Airflow orchestration

Apple Siliconâ€“safe stack

we move to:
â¡ï¸ Silver layer transformations (PySpark + data quality)
â¡ï¸ Then dbt
â¡ï¸ Then Gold analytics models


Silver Layer â€“ What It Does (Precisely)

Input:

Bronze Parquet (raw, minimally processed)

Output:

Clean, typed, trusted Parquet

Partitioned for analytics

Ready for dbt

Silver responsibilities (non-negotiable):

Schema enforcement (NO inference)

Data type normalization

Null & range validation

Business feature derivation

Standard column naming

Reject / filter bad records

Silver is where â€œdata engineeringâ€ actually happens.

2ï¸âƒ£ Silver Data Quality Rules (Explicit)

Assume Uber/TLC-like data.

Hard Rules (Rows failing these are dropped)

pickup_datetime IS NOT NULL

dropoff_datetime IS NOT NULL

trip_distance > 0

fare_amount >= 0

dropoff_datetime >= pickup_datetime

Soft Rules (Clean / standardize)

Cast timestamps to UTC

Cast numeric fields explicitly

Normalize column names to snake_case

Derived Fields (Silver-only)

trip_duration_minutes

pickup_date

pickup_hour

time_of_day (morning/afternoon/evening/night)

4ï¸âƒ£ Why This Is Correct (Important)
âœ… Why explicit schema?

Prevents silent type corruption

Makes failures loud

Interview gold

âœ… Why partition by pickup_date?

Most queries filter by date

Enables predicate pushdown

Reduces scan cost

âœ… Why Spark (not dbt) here?

Row-level logic

Heavy computation

Data quality enforcement

1ï¸âƒ£ What dbt will do in your architecture

Input: Silver Parquet data in MinIO

Engine: Spark (via dbt-spark)

Output: Gold analytics tables (facts & dimensions)

Role: SQL transformations, tests, docs â€” not ingestion

2ï¸âƒ£ Create dbt project (locally, not in Airflow yet)


What you should see now

Inside Spark (not MinIO):

A managed Spark table:

gold.gold_trip_metrics


Queryable via:

dbt

Spark SQL

Thrift clients (Beeline, DBeaver, etc.)

You can confirm via Spark UI:

http://localhost:18080

Why this is portfolio-ready ğŸ”¥

You now have:

Bronze â†’ raw parquet (MinIO)

Silver â†’ cleaned, partitioned parquet

Gold â†’ analytics tables via dbt + Spark

Airflow-ready orchestration

Cloud-style object storage

Zero paid tools

Apple Silicon compatible

This is real data engineering, not a toy project.


âœ… What youâ€™ve proven
1ï¸âƒ£ dbt â†’ Spark Thrift is working

dbt created gold.gold_trip_metrics

Spark registered it in the metastore

Thrift server exposed it

Beeline queried it successfully

This is exactly how enterprises run dbt on Spark.

2ï¸âƒ£ Medallion architecture is complete
Layer	Technology	Verified
Bronze	Raw Parquet in MinIO	âœ…
Silver	Cleaned + partitioned Parquet	âœ…
Gold	dbt â†’ Spark SQL tables	âœ…
Query	Thrift / JDBC	âœ…

Your query output is aggregated analytics, not raw data â€” thatâ€™s the Gold layer done right.

3ï¸âƒ£ Numbers make sense (important)

Trips grouped by pickup_date + time_of_day

Metrics are realistic

No null explosions

Aggregations are stable

This shows your Silver data quality logic worked.

ğŸš€ Portfolio value (this is what matters)

If someone asks:

â€œHave you ever built a lakehouse?â€

You can now say yes and show:

Dockerized infra

Object storage (MinIO)

Spark ETL (Bronze â†’ Silver)

dbt transformations (Silver â†’ Gold)

Spark Thrift + JDBC querying

Apple Silicon compatible setup

That puts you ahead of most applicants.

schema.yml
âœ… What youâ€™ve achieved (important)

Fixed metric correctness (real-world validation)

Added data quality enforcement

Gold layer is now trustworthy + production-grade

This is exactly what interviewers look for.
2ï¸âƒ£ Make Gold incremental
